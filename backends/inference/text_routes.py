def inference_completions(query: str) -> str:
    # @TODO Load the normal text llm completion..
    # Or use llamaIndex's abstraction
    # llm.stream_complete()

    return query
